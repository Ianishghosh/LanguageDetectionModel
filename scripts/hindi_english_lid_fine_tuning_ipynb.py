# -*- coding: utf-8 -*-
"""Hindi_English_LID_Fine_Tuning.ipynb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hi9JI9T8Fre79o8-H7MpYiNByerWJxsF
"""

# Basic mount
from google.colab import drive
drive.mount('/content/drive')  # Authorize in the popup, then paste the code

# Install SpeechBrain and other essentials
!pip install speechbrain
!pip install torchaudio
!pip install pandas
!pip install scipy

# Verify GPU is available
import torch
print("CUDA available:", torch.cuda.is_available())
print("Device:", torch.cuda.get_device_name(0))

# Cell 1: Installations
!pip install speechbrain torchaudio pandas
!add-apt-repository -y universe
!apt update
!apt install -y ffmpeg  # Install FFmpeg for audio conversion
print("All packages installed successfully!")

!apt-get install ffmpeg -y

import os

# Update the input_dir to point to the correct location in your Google Drive
# Replace 'Your_Dataset_Folder' with the actual path to your Datasets folder in Google Drive
input_dir = "/content/Datasets.zip"
for lang in ["en", "hi"]:
    folder = os.path.join(input_dir, lang)
    # Add a check to ensure the folder exists before trying to list its contents
    if os.path.exists(folder):
        for f
ile in os.listdir(folder):
            if file.endswith(".mp3"):
                mp3_path = os.path.join(folder, file)
                wav_path = mp3_path.replace(".mp3", ".wav")
                !ffmpeg -y -i "$mp3_path" -ar 16000 -ac 1 "$wav_path"
    else:
        print(f"Warning: Directory not found: {folder}")

from google.colab import drive
drive.mount('/content/drive')

# Copy from Drive to Colab working dir
!cp /content/drive/MyDrive/Datasets.zip /content/

from google.colab import files
uploaded = files.upload()

# STEP 1: Upload your mini zip
from google.colab import files
uploaded = files.upload()   # choose mini_dataset.zip

# STEP 2: Unzip into /content/data_mini
!rm -rf /content/data_mini
!mkdir -p /content/data_mini
!unzip -q mini_dataset.zip -d /content/data_mini

# Show structure
!find /content/data_mini -maxdepth 2 -type d -print

# STEP 3: Install deps (Transformers + Datasets + audio utils + ffmpeg)
!apt-get -y install ffmpeg
!pip -q install --upgrade pip
!pip -q install "transformers==4.44.0" "datasets==2.19.0" "accelerate==0.33.0" "evaluate==0.4.2" \
                 "librosa==0.10.2.post1" "soundfile==0.12.1" "torch" "torchaudio"

# STEP 4: Convert all MP3 -> WAV (16kHz mono), keep folders en/ and hi/
import os, subprocess, pathlib

root = "/content/data_mini/mini_dataset"  # path you unzipped to
langs = ["en", "hi"]

def convert_dir(mp3_dir):
    for name in sorted(os.listdir(mp3_dir)):
        p = os.path.join(mp3_dir, name)
        if os.path.isfile(p) and p.lower().endswith(".mp3"):
            wav_path = p[:-4] + ".wav"
            cmd = ["ffmpeg", "-y", "-i", p, "-ar", "16000", "-ac", "1", wav_path]
            subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

for lang in langs:
    convert_dir(os.path.join(root, lang))

# Optional: verify counts
for lang in langs:
    path = os.path.join(root, lang)
    wavs = [f for f in os.listdir(path) if f.lower().endswith(".wav")]
    mp3s = [f for f in os.listdir(path) if f.lower().endswith(".mp3")]
    print(lang, "->", len(wavs), "wav files,", len(mp3s), "mp3 files (safe to keep or delete)")

from datasets import load_dataset, Audio

# Load your audio as a folder dataset; labels come from folder names (en, hi)
ds = load_dataset("audiofolder", data_dir="/content/data_mini/mini_dataset", split="train")
# Standardize sampling rate to 16k (we already converted, but cast_column keeps it consistent)
ds = ds.cast_column("audio", Audio(sampling_rate=16000))

# Train/validation split (stratified by label)
ds_split = ds.train_test_split(test_size=0.2, stratify_by_column="label", seed=42)
train_ds = ds_split["train"]
eval_ds  = ds_split["test"]

train_ds, eval_ds

# Installations
!pip install speechbrain torchaudio pandas
!add-apt-repository -y universe
!apt update && apt install -y ffmpeg

# Upload your dataset
from google.colab import files
uploaded = files.upload()  # Upload your mini_dataset.zip here

# Extract it
import zipfile
import os
with zipfile.ZipFile('mini_dataset.zip', 'r') as zip_ref:
    zip_ref.extractall('./')

# Set the CORRECT path
base_path = './mini_dataset'  # This is where your files are now!

# Let's see what's inside
print("Contents of your dataset:")
!find $base_path -type f | head -20

# Now convert MP3 to WAV
import subprocess
from pathlib import Path

def convert_mp3_to_wav(mp3_path, wav_path):
    command = ['ffmpeg', '-i', mp3_path, '-ar', '16000', '-ac', '1', '-y', wav_path]
    subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

# Create processed folder
!mkdir -p ./processed_data

data_list = []
label_map = {'hi': 0, 'en': 1}

for lang in ['hi', 'en']:
    input_dir = os.path.join(base_path, lang)
    output_dir = os.path.join('./processed_data', lang)
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    print(f"Processing {lang} files...")
    for i, filename in enumerate(os.listdir(input_dir)):
        if filename.endswith('.mp3'):
            mp3_path = os.path.join(input_dir, filename)
            wav_filename = f"{lang}_{i}.wav"
            wav_path = os.path.join(output_dir, wav_filename)

            convert_mp3_to_wav(mp3_path, wav_path)
            data_list.append({'wav': wav_path, 'label': label_map[lang]})
            print(f"Converted: {filename} -> {wav_filename}")

print(f"Finished processing {len(data_list)} files.")

# Now continue with the rest of your training code...
# [PASTE THE REST OF YOUR TRAINING CODE HERE]

# ====== PREPARE DATA FOR TRAINING ======
import pandas as pd
from speechbrain.dataio.dataio import read_audio
from speechbrain.dataio.dataset import DynamicItemDataset
from speechbrain.dataio.dataloader import make_dataloader

# Create a DataFrame with file paths and labels
df = pd.DataFrame(data_list)
print("DataFrame created for training:")
print(df.head())

# Create SpeechBrain dataset
def audio_pipeline(wav):
    return read_audio(wav)

def label_pipeline(label):
    return label

dataset = DynamicItemDataset.from_dict(df.to_dict('list'))
dataset.add_dynamic_item(audio_pipeline)
dataset.add_dynamic_item(label_pipeline, output_key="label_encoded")
dataset.set_output_keys(["sig", "label_encoded"])

print("Dataset prepared successfully!")
print(f"Number of training samples: {len(df)}")

# ====== TRAIN THE MODEL ======
from speechbrain.nnet.losses import nll_loss
from speechbrain.core import Brain
import torch
from hyperpyyaml import load_hyperpyyaml
import io

# Model configuration
yaml_config = """
compute_features: !new:speechbrain.processing.features.Fbank
    sample_rate: 16000
    n_mels: 80

mean_var_norm: !new:speechbrain.processing.features.InputNormalization
    norm_type: sentence
    std_norm: False

embedding_model: !new:speechbrain.lobes.models.ECAPA_TDNN.ECAPA_TDNN
    input_size: 80
    lin_neurons: 192

classifier: !new:speechbrain.nnet.linear.Linear
    input_size: 192
    n_neurons: 2

opt_class: !name:torch.optim.Adam
    lr: 0.001
"""

hparams = load_hyperpyyaml(io.StringIO(yaml_config))

class LangIDBrain(Brain):
    def compute_forward(self, batch, stage):
        batch = batch.to(self.device)
        wavs, wav_lens = batch["sig"]
        feats = self.hparams.compute_features(wavs)
        feats = self.hparams.mean_var_norm(feats, wav_lens)
        embeddings = self.modules.embedding_model(feats, wav_lens)
        outputs = self.modules.classifier(embeddings)
        return outputs

    def compute_objectives(self, predictions, batch, stage):
        labels = batch["label_encoded"]
        loss = nll_loss(predictions, labels)
        return loss

# Initialize and train the model
print("Initializing model...")
lang_id_brain = LangIDBrain(
    modules=hparams,
    hparams=hparams,
    run_opts={"device": "cuda" if torch.cuda.is_available() else "cpu"}
)

# Create data loader
train_loader = make_dataloader(dataset, batch_size=2, shuffle=True)

print("Starting training...")
lang_id_brain.fit(
    epoch_counter=range(5),  # Train for 5 epochs
    train_set=train_loader,
)

print("Training completed successfully!")

# ====== TEST THE MODEL ======
# Let's test our model on one of the training files
print("\nTesting the model on a sample file...")
test_file = df.iloc[0]['wav']  # Get the first file
test_label = df.iloc[0]['label']

# Load and prepare the test audio
waveform = read_audio(test_file)
waveform = waveform.unsqueeze(0)  # Add batch dimension

# Make prediction
with torch.no_grad():
    predictions = lang_id_brain.compute_forward({"sig": waveform, "label_encoded": None}, stage=sb.Stage.TEST)
    predicted_label = torch.argmax(predictions, dim=-1).item()

print(f"File: {test_file}")
print(f"True label: {test_label} ({'Hindi' if test_label == 0 else 'English'})")
print(f"Predicted label: {predicted_label} ({'Hindi' if predicted_label == 0 else 'English'})")
print(f"Confidence: {torch.nn.functional.softmax(predictions, dim=-1).max().item():.2%}")

# ====== SAVE THE MODEL ======
lang_id_brain.save_results('trained_model/')
print("\nModel saved to 'trained_model/' directory")

# ====== DOWNLOAD THE MODEL ======
from google.colab import files
!zip -r /content/trained_model.zip /content/trained_model
files.download('/content/trained_model.zip')
print("Download the 'trained_model.zip' file for use in your VS Code project!")

# FIX NUMPY COMPATIBILITY ISSUE
!pip uninstall -y numpy
!pip install numpy==1.21.0
!pip install numba==0.53.1

# Now restart the runtime to apply changes
import os
os.kill(os.getpid(), 9)

# Install required packages (run this first)
!pip install librosa scikit-learn numpy pandas matplotlib seaborn

import os
import numpy as np
import pandas as pd
import librosa
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import joblib  # For saving the model

# Configuration
dataset_path = "./mini_dataset"  # Path to your dataset
languages = ['hi', 'en']  # Your language codes

def extract_features(file_path, max_pad_len=174):
    """
    Extract MFCC features from audio file and pad/truncate to fixed length
    """
    try:
        # Load audio file
        audio, sr = librosa.load(file_path, sr=22050)  # Resample to 22.05 kHz

        # Extract MFCC features (13 coefficients)
        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)

        # Pad or truncate to make all feature arrays the same size
        if mfccs.shape[1] > max_pad_len:
            mfccs = mfccs[:, :max_pad_len]
        else:
            pad_width = max_pad_len - mfccs.shape[1]
            mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')

        return mfccs.flatten()  # Flatten to 1D array for ML models

    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        return None

def create_dataset():
    """
    Create dataset from audio files
    """
    features = []
    labels = []

    for lang in languages:
        lang_path = os.path.join(dataset_path, lang)

        if not os.path.exists(lang_path):
            print(f"Directory {lang_path} does not exist")
            continue

        print(f"Processing {lang} files...")

        for file_name in os.listdir(lang_path):
            if file_name.endswith('.wav') or file_name.endswith('.mp3'):
                file_path = os.path.join(lang_path, file_name)

                # Extract features
                feature_vector = extract_features(file_path)

                if feature_vector is not None:
                    features.append(feature_vector)
                    labels.append(lang)

    return np.array(features), np.array(labels)

# Create the dataset
print("Creating dataset from audio files...")
X, y = create_dataset()

if len(X) == 0:
    print("No features extracted. Please check your dataset path and file formats.")
else:
    print(f"Dataset created with {X.shape[0]} samples and {X.shape[1]} features each")

    # Encode labels (hi -> 0, en -> 1)
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    # Split dataset into training and testing
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

    # Standardize features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Train a model (Random Forest is less prone to overfitting with small datasets)
    print("Training model...")
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train_scaled, y_train)

    # Evaluate the model
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"Model trained with {accuracy*100:.2f}% accuracy")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=le.classes_))

    # Save the model and preprocessing objects
    joblib.dump(model, 'language_detection_model.pkl')
    joblib.dump(scaler, 'scaler.pkl')
    joblib.dump(le, 'label_encoder.pkl')

    print("Model saved as 'language_detection_model.pkl'")
    print("Scaler saved as 'scaler.pkl'")
    print("Label encoder saved as 'label_encoder.pkl'")

    # Download the files
    from google.colab import files
    files.download('language_detection_model.pkl')
    files.download('scaler.pkl')
    files.download('label_encoder.pkl')

!pip install transformers datasets torchaudio librosa
!pip install soundfile
!pip install accelerate -U

# Installations
!pip install speechbrain torchaudio pandas
!add-apt-repository -y universe
!apt update && apt install -y ffmpeg

# Upload your dataset
from google.colab import files
uploaded = files.upload()  # Upload your mini_dataset.zip here

# Extract it
import zipfile
import os
with zipfile.ZipFile('mini_dataset.zip', 'r') as zip_ref:
    zip_ref.extractall('./')

# Set the CORRECT path
base_path = './mini_dataset'  # This is where your files are now!

# Let's see what's inside
print("Contents of your dataset:")
!find $base_path -type f | head -20

# Now convert MP3 to WAV
import subprocess
from pathlib import Path

def convert_mp3_to_wav(mp3_path, wav_path):
    command = ['ffmpeg', '-i', mp3_path, '-ar', '16000', '-ac', '1', '-y', wav_path]
    subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

# Create processed folder
!mkdir -p ./processed_data

data_list = []
label_map = {'hi': 0, 'en': 1}

for lang in ['hi', 'en']:
    input_dir = os.path.join(base_path, lang)
    output_dir = os.path.join('./processed_data', lang)
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    print(f"Processing {lang} files...")
    for i, filename in enumerate(os.listdir(input_dir)):
        if filename.endswith('.mp3'):
            mp3_path = os.path.join(input_dir, filename)
            wav_filename = f"{lang}_{i}.wav"
            wav_path = os.path.join(output_dir, wav_filename)

            convert_mp3_to_wav(mp3_path, wav_path)
            data_list.append({'wav': wav_path, 'label': label_map[lang]})
            print(f"Converted: {filename} -> {wav_filename}")

print(f"Finished processing {len(data_list)} files.")

# Now continue with the rest of your training code...
# [PASTE THE REST OF YOUR TRAINING CODE HERE]

from datasets import Dataset, Audio
import os
from transformers import AutoFeatureExtractor

# Create a dataset from your folders
def create_dataset(base_path):
    data = {"audio": [], "label": []}
    label_map = {"hi": 0, "en": 1}

    for lang in ["hi", "en"]:
        lang_path = os.path.join(base_path, lang)
        for file in os.listdir(lang_path):
            if file.endswith(".wav") or file.endswith(".mp3"):
                data["audio"].append(os.path.join(lang_path, file))
                data["label"].append(label_map[lang])

    return Dataset.from_dict(data)

# Create dataset
dataset = create_dataset("./mini_dataset")

# Split dataset
dataset = dataset.train_test_split(test_size=0.2, seed=42)
print(f"Training samples: {len(dataset['train'])}")
print(f"Test samples: {len(dataset['test'])}")

from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer

model_name = "facebook/wav2vec2-base"  # You can try other models like "facebook/wav2vec2-large" or "microsoft/wavlm-base"

# Load feature extractor
feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)

# Load model
model = AutoModelForAudioClassification.from_pretrained(
    model_name,
    num_labels=2,  # Hindi and English
    label2id={"hi": 0, "en": 1},
    id2label={0: "hi", 1: "en"}
)

def preprocess_function(examples):
    audio_arrays = [x["array"] for x in examples["audio"]]
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=feature_extractor.sampling_rate,
        padding=True,
        return_tensors="pt"
    )
    return inputs

# Preprocess the dataset
encoded_dataset = dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=dataset["train"].column_names
)

# Step 1: Install required libraries (if needed)
!pip install transformers torchaudio
!pip install accelerate
!pip install evaluate scikit-learn

# Step 2: Import all necessary libraries
import os
import torch
import torchaudio
import numpy as np
from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer
import evaluate
from sklearn.metrics import accuracy_score

# Step 3: Prepare Your Dataset
def load_audio_files(base_path):
    """Load all audio files from the dataset"""
    data = {"audio": [], "labels": []}
    label_map = {"hi": 0, "en": 1}

    for lang in ["hi", "en"]:
        lang_path = os.path.join(base_path, lang)
        for file in os.listdir(lang_path):
            if file.endswith(".wav") or file.endswith(".mp3"):
                file_path = os.path.join(lang_path, file)

                # Load audio file
                waveform, sample_rate = torchaudio.load(file_path)

                # Resample to 16kHz if needed
                if sample_rate != 16000:
                    resampler = torchaudio.transforms.Resample(sample_rate, 16000)
                    waveform = resampler(waveform)

                data["audio"].append(waveform.squeeze().numpy())
                data["labels"].append(label_map[lang])

    return data

# Load audio data
audio_data = load_audio_files("./mini_dataset")
print(f"Loaded {len(audio_data['audio'])} audio samples")

# Step 4: Load Pre-trained Model and Feature Extractor
model_name = "facebook/wav2vec2-base"

# Load feature extractor
feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)

# Load model
model = AutoModelForAudioClassification.from_pretrained(
    model_name,
    num_labels=2,
    label2id={"hi": 0, "en": 1},
    id2label={0: "hi", 1: "en"}
)

# Step 5: Preprocess Audio Data
def preprocess_audio(audio_arrays):
    """Preprocess audio arrays using the feature extractor"""
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=16000,
        padding=True,
        return_tensors="pt",
        max_length=16000 * 5,  # Limit to 5 seconds of audio
        truncation=True
    )
    return inputs

# Preprocess all audio data
inputs = preprocess_audio(audio_data["audio"])
labels = torch.tensor(audio_data["labels"])

# Step 6: Fine-tune the Model
# Split data into train and test
train_size = int(0.8 * len(audio_data["audio"]))
train_inputs = {k: v[:train_size] for k, v in inputs.items()}
train_labels = labels[:train_size]
test_inputs = {k: v[train_size:] for k, v in inputs.items()}
test_labels = labels[train_size:]

# Create a custom dataset
class AudioDataset(torch.utils.data.Dataset):
    def __init__(self, inputs, labels):
        self.inputs = inputs
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.inputs.items()}
        item["labels"] = self.labels[idx]
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = AudioDataset(train_inputs, train_labels)
test_dataset = AudioDataset(test_inputs, test_labels)

# Step 7: Define compute_metrics function
def compute_metrics(eval_pred):
    predictions = np.argmax(eval_pred.predictions, axis=1)
    return {"accuracy": accuracy_score(eval_pred.label_ids, predictions)}

# Set up training arguments
training_args = TrainingArguments(
    output_dir="./language_detection_model",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=3e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=10,
    warmup_ratio=0.1,
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    push_to_hub=False,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    processing_class=feature_extractor, # Corrected tokenizer to processing_class
    compute_metrics=compute_metrics
)

# Start training
print("Starting fine-tuning...")
trainer.train()

# Save the model
trainer.save_model()
print("Model saved!")

from google.colab import files
import shutil

# Create a zip file of your model
shutil.make_archive('language_detection_model', 'zip', './language_detection_model')

# Download it
files.download('language_detection_model.zip')
print("Model downloaded! Save this file for your live detection system.")

# Create a zip file with only the essential model files
import os
import shutil
from google.colab import files

# Create a temporary directory for the essential files
essential_model_dir = '/content/essential_model/'
os.makedirs(essential_model_dir, exist_ok=True)

# Copy only the essential files
essential_files = [
    'config.json',
    'pytorch_model.bin',
    'preprocessor_config.json',
    'special_tokens_map.json',
    'tokenizer_config.json',
    'vocab.json'  # This might also be needed depending on your tokenizer
]

# Path to your trained model directory
model_dir = '/content/language_detection_model/'

for file in essential_files:
    src_path = os.path.join(model_dir, file)
    if os.path.exists(src_path):
        shutil.copy(src_path, essential_model_dir)
        print(f"Copied: {file}")
    else:
        print(f"File not found: {file}")

# Create a zip file
shutil.make_archive('essential_language_model', 'zip', essential_model_dir)

# Download the zip file
files.download('essential_language_model.zip')

print('Downloaded essential model files!')

import os
import shutil
from google.colab import files

# Path to your model directory
model_dir = '/content/language_detection_model'

# Check what's in the model directory
if os.path.exists(model_dir):
    print("Files in model directory:", os.listdir(model_dir))
else:
    print("Model directory doesn't exist!")
    # If it doesn't exist, you might need to save your model first
    # model.save_pretrained('/content/language_detection_model')
    # processor.save_pretrained('/content/language_detection_model')

# Create a directory for the essential files
essential_dir = '/content/essential_model'
os.makedirs(essential_dir, exist_ok=True)

# Copy only the essential files from the model directory
essential_files = ['config.json', 'pytorch_model.bin', 'preprocessor_config.json',
                  'special_tokens_map.json', 'tokenizer_config.json']

for file in essential_files:
    src_path = os.path.join(model_dir, file)
    if os.path.exists(src_path):
        shutil.copy(src_path, essential_dir)
        print(f"✓ Copied: {file}")
    else:
        print(f"✗ Missing: {file}")

# Create zip and download
shutil.make_archive('essential_language_model', 'zip', essential_dir)
files.download('essential_language_model.zip')

print("Essential model files packaged and downloaded!")

import os
import shutil
from google.colab import files

# Path to your model directory
model_dir = '/content/language_detection_model'

# Check if model.safetensors exists and its size
safetensors_path = os.path.join(model_dir, 'model.safetensors')
if os.path.exists(safetensors_path):
    file_size = os.path.getsize(safetensors_path)
    print(f"✓ model.safetensors exists, size: {file_size} bytes")
else:
    print("✗ model.safetensors doesn't exist!")

# Create a directory for the essential files
essential_dir = '/content/essential_model'
os.makedirs(essential_dir, exist_ok=True)

# List of files to copy (including tokenizer files)
files_to_copy = [
    'config.json',
    'model.safetensors',
    'preprocessor_config.json',
    'training_args.bin',
    'tokenizer_config.json',  # Add tokenizer config
    'special_tokens_map.json', # Add special tokens map
    'vocab.json'              # Add vocab file if it exists
]

# Copy files
for file in files_to_copy:
    src_path = os.path.join(model_dir, file)
    if os.path.exists(src_path):
        shutil.copy(src_path, essential_dir)
        print(f"✓ Copied: {file}")
    else:
        print(f"✗ Missing: {file}")

# Verify files were copied to essential_dir
print("\nFiles in essential directory:")
for f in os.listdir(essential_dir):
    print(f"  - {f}")

# Create zip and download
shutil.make_archive('essential_language_model', 'zip', essential_dir)
files.download('essential_language_model.zip')

print("\nZip file created and downloaded!")

